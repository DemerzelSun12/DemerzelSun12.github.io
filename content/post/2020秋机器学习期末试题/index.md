---
title: 2020秋机器学习期末试题
draft: false
math: true
date: 2020-11-22 22:01:42
cover: images/featureimages/10.jpg
summary: 2020秋季学期机器学习期末试题
tags: [学长的火炬,机器学习]
categories: [学长的火炬]
---


## 前言

今年的题目相对于去年(2019秋)的要简单许多，不过有好多类似的题目。

## 2019秋机器学习期末考试题目

1. 信息增益是什么？举例说明为什么信息增益好。决策树分界面是否为线性，举例说明。
2. 最小错误概率下的最优分类方法并证明。
3. 什么是过拟合，决策树和SVM如何避免过拟合。
4. 在第二题的分类方法下，假设类概率分布式伯努利分布，类条件概率分布是协方差相等的两个高斯分布，求分界面方程，求类后验概率。
5. 逻辑回归优化目标函数。从交叉熵的角度理解，交叉熵的两个概率分布分别是什么？
6. PCA推导，PCA应用举例(2个)。
7. GMM模型优化目标函数，EM算法主要步骤。在做实验时，协方差矩阵行列式为0的原因，如何解决。
8. 在低维空间中线性不可分的样本，为什么在高维空间中线性可分，举例说明，在实验中有无应用？
9. 机器学习中内积的应用举例(3个)，解释他们的意义。

## 2020秋机器学习期末考试题目

1. 样本的类别标签Y和某个属性$A_1$（二者可看成随机变量），二者之间的互信息在决策树构建中可以指导选择属性。
- 互信息的定义？
- 依赖互信息（信息增益）选择属性的目的？有什么好处？
- 如何避免决策树的过拟合？
2. Y为表示样本类别的随机变量（假定2类，$Y=0$，$Y=1$），X为样本样例特征向量，对X的错误分类会导致损失，将真实类为1的样本判断为类0造成的损失为a，反之为b，如下表所示。

<center>
   <table class="tg">
      <tbody>
        <tr>
            <th class="tg-c3ow">Y</th>
            <th class="tg-0pky">0</th>
            <th class="tg-0pky">1</th>
        </tr>
         <tr>
            <th class="tg-c3ow">0</th>
            <td class="tg-0pky">0</td>
            <td class="tg-0pky">a</td>
         </tr>
         <tr>
            <th class="tg-c3ow">1</th>
            <td class="tg-0pky">b</td>
            <td class="tg-0pky">0</td>
         </tr>
      </tbody>
   </table>
</center>

   - 期望损失最小意义下的分类准则是什么？
   - 用图例说明之。

3. 三问
- 给出线性可分时SVM优化的目标函数和约束条件。
- SVM对线性不可分数据的解决方案。
- SVM中支持向量有什么意义。

4. 三问
- 本课程中模型参数估计主要有哪些方法？
- 它们的区别？
- 什么情况下会获得趋于一致的结果。

5. 两问
- 为什么Logistic优化的目标函数为条件似然$\log \left(Y|X\right) $，而非$\log \left(X,Y\right)$？
- 推导如何从Logistic回归获得样本空间的决策面。

6. 两问
- 从信号重建的角度推导PCA；
- 如何使用PCA压缩信号？

7. 三问
- k-Means算法流程；
- 给出K-Means的优化目标函数；
- 与EM算法的相似性与不同点。

8. 本课程第一个实验是单变量非线性回归问题，若把其看做线性回归问题，用公式给出你的解决方案。
